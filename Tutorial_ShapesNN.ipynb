{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial-ShapesNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbeasley1998/NINEworkshop/blob/master/Tutorial_ShapesNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LgFXWw09O2Av"
      },
      "source": [
        "<h1 style=\"text-align:center; color:navy\";> Shapes Neural Network </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcSpF5d-CPfx",
        "colab_type": "text"
      },
      "source": [
        "<h2 style=\"text-align:left; color:SeaGreen\";>  Table of Contents </h2>\n",
        " \n",
        "1. [Classifiers](#one)\n",
        "\n",
        "2. [Intro](#two)\n",
        "\n",
        "3. [Neural Netoworks](#three)\n",
        "   \n",
        "4 [Building our Neural Network](#four)\n",
        "    <br> 7.1 [Imports and Objectives](#four1)\n",
        "    <br> 7.2 [Importing Data](#four2) \n",
        "    <br> 7.3 [Define Neural Network](#four3)\n",
        "    <br> 7.4 [Define Loss Function and Optimizer](#four4)\n",
        "    <br> 7.5 [Training Loop](#four5)\n",
        "    <br> 7.6 [Model Evaluation](#four6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1WDlCjSCPfy",
        "colab_type": "text"
      },
      "source": [
        "<h2 style=\"text-align:left; color:SeaGreen\";> Classifiers\n",
        "<a id=\"one\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "quK8A_UgWC4E"
      },
      "source": [
        "   <p style=\"text-indent: 50px; font-size:16px;\">Classification, in machine learning terms, is the act of taking an input, and grouping it with other like inputs. These groups of similar inputs are called 'Classes' and assigning an input to a class is 'Classifying'. <p>\n",
        "\n",
        "  <p style=\"text-indent: 50px; font-size:16px;\"> A more digestable way to look at it would be that you give the machine examples of what something is, and it takes these examples and tries to find patterns that help it identify other instances of this thing. <p>\n",
        "\n",
        "   <p style=\"text-indent: 50px; font-size:16px;\"> Give it 100 pictures of dogs and tell it that those are dogs. Give it 100 pictures of a washing machine and tell it those are washing machines. The idea is that with the 100 examples of each thing, the AI can build an understanding of what a 'dog' looks like and what a 'washing machine' looks like, and it calls these labels 'classes'. You can now give the AI a new picture of either thing and hope that the has enough information to know what it is without you telling it. Below you can find an article that gives some useful info on classifiers, namely what types of data they can use as inputs. Reading it in full will not be pertenent for this notebook, but scrolling through can give some idea of what types of problems can be solved by a classifier. <p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VUP3pbPCPfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "21d826e9-fd4d-469c-e13f-6bb1ec9a85f9"
      },
      "source": [
        "#Website Display Don't Edit\n",
        "from IPython.display import IFrame\n",
        "IFrame('https://machinelearningmastery.com/types-of-classification-in-machine-learning/#:~:text=In%20machine%20learning%2C%20classification%20refers,one%20of%20the%20known%20characters.', width=1000, height=600)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"600\"\n",
              "            src=\"https://machinelearningmastery.com/types-of-classification-in-machine-learning/#:~:text=In%20machine%20learning%2C%20classification%20refers,one%20of%20the%20known%20characters.\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f02f5355be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fqmBqeACPf2",
        "colab_type": "text"
      },
      "source": [
        "<h2 style=\"text-align:left; color:SeaGreen\";> Shapes Neural Network Intro </h2> \n",
        "<a id=\"two\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aiiD3fvDdGmX"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\">The classifier that will be built throughout the length of this notebook will be able to classify shapes. Cirlces, squares, triangles, and stars will be our target shapes and thus we will make four classes out of them. This will be like using the toy seen below the title of this notebook, only OUR baby is the best damn shape knowing child in history. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfuAPXLmlR0F"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\">The data set we will use will contain 2400 images of shapes, that is 600 images x our 4 classes. As we mentioned above, we need to give many of these images to our neural network in order for it to learn what each shape looks like. In data science this is very typically called a <b>Training Data Set</b>. It is the portion of the data you give to the algorithm where the class is told to it, and it tries to notice patterns among data in the same classes. These patterns are what help the AI 'know' what each class shape looks like. We also need to reserve a smaller portion of data to test the alroritm on what it has learned. This is often called a <b>Testing Data Set</b>. The classes for the testing portion of data are known to us, but are not told to the AI. We will give the AI a random picture from the testing set, and see if it can classify it under the correct class. The download for the dataset can be found here (INSERT DATA DISTRIBUTION WHEN FIGURED OUT). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hLqCSD2j-X4"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\">For the best results, you want to try and split your data into the training/testing sets evenly and consistently. <b>For each shape class, we will have 500 pictures for training, and 100 pictures for testing.</b> The folder we downloaded of all the images should look like this in your directory: \n",
        "<img src=\"https://i.imgur.com/4JkxPX9.png\" width=200, height = 200> </p>\n",
        "\n",
        "<p style=\"text-indent: 50px; font-size:16px;\"> Here, there should be 500 images in each folder in the 'train' foler, and 100 images in each folder in the 'valid' folder. <b> Once we have trained our alorithm, hopefully it will be able to put any shape image from the 'valid' folder into the correct class without being told. </b> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UD7w2CzGlTqE"
      },
      "source": [
        "<h2 style=\"text-align:left; color:SeaGreen\";> Neural Networks and PyTorch </h2> \n",
        "<a id=\"three\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ejyf__T57e",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> The classifier we have been discussing, and are about to build, will be made using a neural network. Describing it might sound familiar to the description of what a classifier is: it is a system of learning used by the computer to create relationships betweeen inputs and their outputs, and uses these relationships to make predictions on other inputs. The major difference is that a classifier aims to simply organize an input into its correct class, where a neural network in general can make far more complex and un-defined predictions. Classifiers are either right or wrong, where Neural Networks can output probablities on many possible outcomes. Squares are to rectangles what classifiers are neural netowrks. The video below can go into more detail. </p> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuGD6noT-hU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYrcNLTdCPf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('GvQwE2OhL8I', width=1000, height=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-JwD79tCPf9",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> Building an AI requires use of a tool or library in order split the data, train the model, and run tests. There are many libraries to choose from, and choosing the right one is situational. Pytorch is the library that we will use for this specific project, as it is currently the industry standard. </p>\n",
        "\n",
        "More information on Pytorch can be found on their website [here](https://pytorch.org/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FlMETiGmfCIt"
      },
      "source": [
        "<h2 style=\"text-align:left; color:SeaGreen\";> Building our Neural Network </h2>\n",
        "<a id=\"four\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRKf-4LuCPf-",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"color:LightSeaGreen\"> Imports and Objectives </h3> \n",
        "<a id=\"four1\"></a>\n",
        "\n",
        "<p style=\"text-indent: 50px; font-size:16px;\"> First, we have to inport all of the relevant packages for our neural network. This can be used as a good copy/paste material for building other neural networks using Pytorch. That being said, the packages you need from the Pytorch library are dependent on your goals, much like deciding upon the Pytorch library to begin with. To know what we need to import, we will create a list of the objectives that will guide our decisions. You don't want to over import too many useless packages as they may slow performance. A basic overview of our goals is as follows: </p>\n",
        " \n",
        "<u> Objectives </u>\n",
        "- Load the data, separate it into testing and training sets, and convert it to tensors\n",
        "- Define our neural networks' class, consisting of the init and forward functions \n",
        "- Set up an optimizer and fit it to our model \n",
        "- Create a loss function that can output our losses to evaluate our model, and plot it\n",
        "- Run the model throuh X number of epochs until acceptably accurate \n",
        "\n",
        "<p style=\"text-indent: 50px; font-size:16px;\"> Now, lets go into a python cell and import all the necessary packages. We will comment on the imports based on what they are used for, but in general this can be re-used and edited for imports on other projects.</p> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLunJGWZCPf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Basic Imports\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import time \n",
        "\n",
        "#Neural Network Imports \n",
        "import torch #importing pytorch library \n",
        "from torch import optim #used to create the optimizer \n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable  \n",
        "from torch.utils.data import Dataset,DataLoader #help in loading and splitting the data into testing and training sets \n",
        "\n",
        "import torch.nn as nn #imports the skeleton (module) of our neural network  \n",
        "import torch.nn.functional as F #imports the functions we need to create our neural network \n",
        "\n",
        "import torchvision #helps importing and preparing the data \n",
        "from torchvision import transforms #helps in transfomring the data \n",
        "from torchvision import models \n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-5LhVb8CPgA",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"color:LightSeaGreen\">Importing the Data </h3> \n",
        "<a id=\"four2\"></a>\n",
        "\n",
        "\n",
        "<p style=\"text-indent: 50px; font-size:16px;\"> First off, we are going to define a function called simple_transform below. This will be used as a way to import our data in a methodology that the computer can 'see'. This means creating a [tensor](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html\n",
        ") which will consist of layers of numbers that represent sections of the picture. </p>\n",
        "\n",
        "<p style=\"text-indent: 50px; font-size:16px;\"> Note the resize function that is changing the picture to 64X64 and the 'To.Tensor()' function in the code that is, predictably, transforming the pictures to tensors. </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aMW5rJRzG2RC",
        "colab": {}
      },
      "source": [
        "simple_transform = transforms.Compose([transforms.Resize((64, 64))\n",
        "                                       ,transforms.ToTensor()\n",
        "                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ac5MkfCPgD",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\">  Now, with the tansform function in hand, we are going to load the data into their respective training and testing sets. You must access the images as they exist in your directory. Placing your cursor over the folders in your files, right clicking, and picking properties can show you the path to your images. The second python cell will confirm that our data has been converted to the tensors. </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "81YRkSJUfhbg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "21e471d5-9534-49c0-ac6d-a69a6e044836"
      },
      "source": [
        "train = ImageFolder('/srv/data/my_shared_data_folder/images-sample/images/train/', simple_transform)\n",
        "valid = ImageFolder('/srv/data/my_shared_data_folder/images-sample/images/valid/', simple_transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b7351f2d2745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D:/nine2020/images-sample/images/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D:/nine2020/images-sample/images/valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    204\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/nine2020/images-sample/images/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0F9N6CYCPgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf0WGOe_CPgI",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\">Because of the way we divided our images into folders, we have primed PyTorch to automatically identify the classes of each image based on the folder names. Here you can see the class names as we see them, then as the numbers the computer sees them. Converting the classes to numbers makes the tasks easier on the computer, and this is called 'indexing'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-AYIGBpAfxM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "d62c1486-f6d2-4816-8af9-9c2abd2c1bec"
      },
      "source": [
        "train.classes\n",
        "\n",
        "train.class_to_idx\n",
        "\n",
        "for key, value in train.class_to_idx.items():\n",
        "    print(\"Class: {!s}  Value: {!s}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-14188dde3b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SP2qN17CPgK",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"color:LightSeaGreen\"> Defining Neural Network </h3> \n",
        "<a id=\"four3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1d6l8jMJkriY"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> Now that our data is ready, we need to define our neural network. This will done by creating a class called 'net'. This class will contain two functions that the network will use to move data through its levels of convolution. The init function creates the layers of convolution for the netowrk to run through. The forward function defines how the data will go through these levels. At the bottom of our class definition we will create a new model of this class called 'model'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6whUyX0WlORa",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Two convolution kernels\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(3380, 50)\n",
        "        self.fc2 = nn.Linear(50,4)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "    \n",
        "model = Net()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifuMGk1MCPgO",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"color:LightSeaGreen\"> Defining the Optimizer and Loss Function </h3> \n",
        "<a id=\"four4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4AszYREQlR4w"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> When the data runs through the 'net' class created above, we call that one epoch. Every epoch is assesed by a 'loss function' that describes how effectively the network was able to make predictions on the models. The output of the forward function needs to be read, assesed, and ran back through the same net class. Before it can do this, some changes need to be made to the Network's convolutional layers. Namely, there needs to be adjustments to features like the weight and learning rate in the network so that each time the data is run through the 'net' class it has more accurate predictions than the last. This task is done by an 'optimizer' which will be created and defined below. The optimizer uses what is retuned by the loss funtion in order to direct its optimization of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FmCyVTeZC9-R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6472744c-5777-442c-c621-270e54d2e151"
      },
      "source": [
        "lossfun = F.nll_loss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-68ea58474983>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K8nohojvloNh"
      },
      "source": [
        "<h3 style=\"color:LightSeaGreen\"> Creating the Training Loop </h3>\n",
        "<a id=\"four5\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlsqEA2UCGum",
        "colab": {}
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> Now we have everything set up to train out model and have it learn what shapes are what. We need to create a loop that will run the data through the net class, calclulate the loss, and optimize the network based on that loss. There is a lot of code in the next cell, but understanding that basic flow of data that has been described is all that matters at the current moment. The data is run through the net() class, a loss function is run on the results, and the optimizer reads that loss and changes the network accordingly. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hOou1stzDxAF",
        "colab": {}
      },
      "source": [
        "def fit(epoch, model, data_loader, phase='training', volatile=False):\n",
        "    if phase == 'training':\n",
        "        model.train()\n",
        "    if phase == 'validation':\n",
        "        model.eval()\n",
        "        volatile = True\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        data, target = Variable(data, volatile), Variable(target)\n",
        "        if phase == 'training':\n",
        "            optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = lossfun(output, target)\n",
        "\n",
        "        #running_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
        "        running_loss += lossfun(output, target, size_average=False).data\n",
        "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "        if phase == 'training':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    loss = running_loss / len(data_loader.dataset)\n",
        "    accuracy = 100. * running_correct / len(data_loader.dataset)\n",
        "\n",
        "    print(f'{phase} loss is {loss} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)} = {accuracy}')\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ltu6PG1CPgV",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"text-align:left; color:LightSeaGreen\";> Evaluating the Model </h3> \n",
        "<a id=\"four6\"></a>\n",
        "    \n",
        "    \n",
        "<p> Creating the proper lists necessary to run a for loop that will take the model through however many epochs specified. At first, we will try three epochs and see the result the loss function outputs. For a simple neural network such as this one, it would be best to aim for above 95% accuracy on the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xrGImQm9DOSV",
        "colab": {}
      },
      "source": [
        "#number of epochs\n",
        "numofepochs = 3\n",
        "\n",
        "#loading\n",
        "train_data_loader = torch.utils.data.DataLoader(train,batch_size=32,num_workers=3,shuffle=True)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid,batch_size=32,num_workers=3,shuffle=True)\n",
        "\n",
        "#defining lists for loss containment \n",
        "train_losses, train_accuracy = [], []\n",
        "val_losses, val_accuracy = [], []\n",
        "\n",
        "#running epochs through \n",
        "for epoch in range(1,numofepochs):\n",
        "    # Train our model\n",
        "    # Validation\n",
        "    # Update our lists\n",
        "    epoch_loss, epoch_accuracy = fit(epoch, model, train_data_loader, phase='training')\n",
        "    val_epoch_loss, val_epoch_accuracy = fit(epoch, model, valid_data_loader, phase='validation')\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracy.append(epoch_accuracy)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    \n",
        "    \n",
        "\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'bo', label='training loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 'r', label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x70t-JwQc8cW"
      },
      "source": [
        "<p style=\"text-indent: 50px; font-size:16px;\"> Based on the results from those three epochs, run the block of code again but change the number of epochs to something higher, like 20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4XwKMsDCPgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of epochs\n",
        "numofepochs = 20\n",
        "\n",
        "#loading\n",
        "train_data_loader = torch.utils.data.DataLoader(train,batch_size=32,num_workers=3,shuffle=True)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid,batch_size=32,num_workers=3,shuffle=True)\n",
        "\n",
        "#defining lists for loss containment \n",
        "train_losses, train_accuracy = [], []\n",
        "val_losses, val_accuracy = [], []\n",
        "\n",
        "#running epochs through \n",
        "for epoch in range(1,numofepochs):\n",
        "    # Train our model\n",
        "    # Validation\n",
        "    # Update our lists\n",
        "    epoch_loss, epoch_accuracy = fit(epoch, model, train_data_loader, phase='training')\n",
        "    val_epoch_loss, val_epoch_accuracy = fit(epoch, model, valid_data_loader, phase='validation')\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracy.append(epoch_accuracy)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    \n",
        "    \n",
        "\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'bo', label='training loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 'r', label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}